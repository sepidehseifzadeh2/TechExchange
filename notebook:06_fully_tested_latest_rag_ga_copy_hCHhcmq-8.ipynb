{"cells": [{"metadata": {}, "id": "a49afec1", "cell_type": "markdown", "source": "# Introduction\nAbout Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires these steps:\n\nExtract knowledge base passages from documents (once)\nCreate vector embedding representations of each passage in the knowledge base\nRetreive question from end user and generate vector embedding for it.\nRetrieve relevant passage(s) from knowledge base (for every user query) using vector similarity search\nGenerate a response by feeding retrieved passage into a large language model (for every user query)"}, {"metadata": {}, "id": "81947875", "cell_type": "markdown", "source": "## Embeddings and Vector Databases\nThe current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n\nWe can generate dense vector representations using embedding models. In this notebook, we use SentenceTransformers all-MiniLM-L6-v2 to embed both the knowledge base passages and user queries. all-MiniLM-L6-v2 is a performant open-source model that is small enough to run locally.\n\nA vector database is optimized for dense vector indexing and retrieval. This notebook uses Chroma, a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with the all-MiniLM-L6-v2 embedding model.\n\nTo generate the final response to a query based on the retrieved passage, we leverage an open-source model, Flan-UL2 (20B), and include a prompt"}, {"metadata": {}, "id": "b1df3001", "cell_type": "markdown", "source": "### About the example dataset\nThe dataset used in this cookbook is a subset of nq_open, an open-source question answering dataset based on contents from Wikipedia. The selected subset includes the gold standard passages to answer the queries in the dataset, which enables evaluating the retrieval quality.\n\nYou can select one of the two dataset available:\n\nnq910 - an information retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\nLongNQ - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive, longer-form question answering. The answers were modified for fluency by IBM Research.\nThese datasets are available in the data assets.\n\nLimitations\nGiven that we are leveraging a locally-hosted embedding model, data ingestion and querying speeds can be slow.\n\nCookbook Structure\nSet-up dependencies\nIndex knowledge base\nGenerate a retrieval-augmented response\nEvaluate RAG performance on your data\n"}, {"metadata": {}, "id": "b61a2a77", "cell_type": "markdown", "source": "#### Disclaimer\nThe IBM GenAI Python library used in this notebook is currently in Beta and will change in the future.\n\n##### 1.1 Install the required dependencies\n\nNote that `ibm-generative-ai` requires `python>=3.9`. Ensure these pre-requisites are met before using this notebook"}, {"metadata": {}, "id": "26f46269", "cell_type": "code", "source": "!pip install chromadb==0.4.5\n!pip install ibm-watson-machine-learning==1.0.311\n!pip install langchain==0.0.261\n!pip install rouge==1.0.1\n!pip install sentence-transformers==2.2.2\n!pip install wget", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting chromadb==0.4.5\n  Downloading chromadb-0.4.5-py3-none-any.whl (402 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting posthog>=2.4.0\n  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\nCollecting fastapi<0.100.0,>=0.95.2\n  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tqdm>=4.65.0\n  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting importlib-resources\n  Downloading importlib_resources-6.0.1-py3-none-any.whl (34 kB)\nCollecting onnxruntime>=1.14.1\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pulsar-client>=3.1.0\n  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting overrides>=7.3.1\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nCollecting pydantic<2.0,>=1.9\n  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb==0.4.5) (1.23.1)\nCollecting chroma-hnswlib==0.7.2\n  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting typing-extensions>=4.5.0\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb==0.4.5) (2.31.0)\nCollecting pypika>=0.48.9\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting tokenizers>=0.13.2\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting starlette<0.28.0,>=0.27.0\n  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.5) (3.19.6)\nCollecting coloredlogs\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.5) (21.3)\nCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.5) (2.0)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: python-dateutil>2.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.5) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.5) (1.16.0)\nCollecting backoff>=1.10.0\n  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.5) (2023.7.22)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.5) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.5) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.5) (1.26.11)\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.5) (8.0.4)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting python-dotenv>=0.13\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting watchfiles>=0.13\n  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httptools>=0.5.0\n  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.5) (6.0)\nCollecting websockets>=10.4\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting anyio<5,>=3.4.0\n  Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting humanfriendly>=9.1\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging->onnxruntime>=1.14.1->chromadb==0.4.5) (3.0.9)\nCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nCollecting exceptiongroup\n  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: chroma-hnswlib, pypika\n  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=181459 sha256=8a9827733472c5122b9ab81953cb6954392ce0f1f8acd692a4637e285008b613\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53738 sha256=ccad1652532b43b8aadef566227421da83a8ca7b32981a3b2713a47404c99c57\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built chroma-hnswlib pypika\nInstalling collected packages: tokenizers, pypika, mpmath, monotonic, websockets, uvloop, typing-extensions, tqdm, sympy, sniffio, python-dotenv, pulsar-client, overrides, importlib-resources, humanfriendly, httptools, h11, exceptiongroup, chroma-hnswlib, backoff, uvicorn, pydantic, posthog, coloredlogs, anyio, watchfiles, starlette, onnxruntime, fastapi, chromadb\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Uninstalling typing_extensions-4.3.0:\n      Successfully uninstalled typing_extensions-4.3.0\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.0\n    Uninstalling tqdm-4.64.0:\n      Successfully uninstalled tqdm-4.64.0\nSuccessfully installed anyio-3.7.1 backoff-2.2.1 chroma-hnswlib-0.7.2 chromadb-0.4.5 coloredlogs-15.0.1 exceptiongroup-1.1.2 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 importlib-resources-6.0.1 monotonic-1.6 mpmath-1.3.0 onnxruntime-1.15.1 overrides-7.4.0 posthog-3.0.1 pulsar-client-3.2.0 pydantic-1.10.12 pypika-0.48.9 python-dotenv-1.0.0 sniffio-1.3.0 starlette-0.27.0 sympy-1.12 tokenizers-0.13.3 tqdm-4.66.1 typing-extensions-4.7.1 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\nRequirement already satisfied: ibm-watson-machine-learning==1.0.311 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (1.0.311)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (2.31.0)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (0.3.3)\nRequirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (2.12.0)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (21.3)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (1.26.11)\nRequirement already satisfied: pandas<1.6.0,>=0.24.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (1.4.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (4.11.3)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (0.8.10)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (2023.7.22)\nRequirement already satisfied: jmespath<1.0.0,>=0.10.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (0.10.0)\nRequirement already satisfied: ibm-cos-sdk-core==2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (2.12.0)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (2.12.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk-core==2.12.0->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning==1.0.311) (2022.1)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning==1.0.311) (1.23.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->ibm-watson-machine-learning==1.0.311) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->ibm-watson-machine-learning==1.0.311) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from importlib-metadata->ibm-watson-machine-learning==1.0.311) (3.8.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from lomond->ibm-watson-machine-learning==1.0.311) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging->ibm-watson-machine-learning==1.0.311) (3.0.9)\nCollecting langchain==0.0.261\n  Downloading langchain-0.0.261-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.261) (4.0.1)\nCollecting aiohttp<4.0.0,>=3.8.3\n  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0\n  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.261) (6.0)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.261) (2.31.0)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.261) (1.23.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.261) (1.4.39)\nCollecting langsmith<0.1.0,>=0.0.11\n  Downloading langsmith-0.0.22-py3-none-any.whl (32 kB)\nCollecting numexpr<3.0.0,>=2.8.4\n  Downloading numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dataclasses-json<0.6.0,>=0.5.7\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nRequirement already satisfied: pydantic<2,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.261) (1.10.12)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.261) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.261) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.261) (1.8.1)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.261) (2.0.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.261) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.261) (5.2.0)\nRequirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from async-timeout<5.0.0,>=4.0.0->langchain==0.0.261) (4.7.1)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.261) (1.26.11)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.261) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.261) (2023.7.22)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.261) (1.1.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.261) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.261) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.261) (3.0.9)\nInstalling collected packages: typing-inspect, tenacity, numexpr, openapi-schema-pydantic, marshmallow, langsmith, aiohttp, dataclasses-json, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Uninstalling tenacity-8.0.1:\n      Successfully uninstalled tenacity-8.0.1\n  Attempting uninstall: numexpr\n    Found existing installation: numexpr 2.8.3\n    Uninstalling numexpr-2.8.3:\n      Successfully uninstalled numexpr-2.8.3\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.8.1\n    Uninstalling aiohttp-3.8.1:\n      Successfully uninstalled aiohttp-3.8.1\nSuccessfully installed aiohttp-3.8.5 dataclasses-json-0.5.14 langchain-0.0.261 langsmith-0.0.22 marshmallow-3.20.1 numexpr-2.8.5 openapi-schema-pydantic-1.2.4 tenacity-8.2.2 typing-inspect-0.9.0\nCollecting rouge==1.0.1\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from rouge==1.0.1) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\nCollecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.12.1)\nRequirement already satisfied: torchvision in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.13.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.1)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.1.1)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.8.1)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.96)\nCollecting huggingface-hub>=0.4.0\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.7.1)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2021.10.1)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nCollecting filelock\n  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\nCollecting regex!=2019.12.17\n  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting safetensors>=0.3.1\n  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.1.1)\nRequirement already satisfied: click in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (8.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (2.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.11)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=8e6835d132eb5b097014b4282fd0a45128e30d375873c51d37b917cacdce462b\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\n", "name": "stdout"}, {"output_type": "stream", "text": "Installing collected packages: safetensors, regex, filelock, nltk, huggingface-hub, transformers, sentence-transformers\nSuccessfully installed filelock-3.12.2 huggingface-hub-0.16.4 nltk-3.8.1 regex-2023.8.8 safetensors-0.3.2 sentence-transformers-2.2.2 transformers-4.31.0\nCollecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9673 sha256=348a09e51e215a84cf02784f6bcd81cad944424f6d7e4bef4802bf5472c081c7\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n", "name": "stdout"}]}, {"metadata": {}, "id": "43427a44", "cell_type": "code", "source": "import os\nfrom getpass import getpass\nfrom typing import Optional, Any, Iterable, List\n\nimport wget\nimport pandas as pd\nimport chromadb\nfrom langchain.vectorstores import Chroma\nfrom sentence_transformers import SentenceTransformer\nfrom chromadb.api.types import EmbeddingFunction\nfrom rouge import Rouge\n\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams", "execution_count": 8, "outputs": []}, {"metadata": {}, "id": "3f39428b", "cell_type": "markdown", "source": "#### 1.3. Load credentials for `ibm-watson-machine-learning`\n\n\n```\nAPI_KEY=<your-api_key>\nIBM_CLOUD_URL=<your-url>\nPROJECT_ID=<your-project_id>\n```"}, {"metadata": {}, "id": "d6a968b7", "cell_type": "code", "source": "IBM_CLOUD_API_KEY = getpass(\"Enter your IBM CLoud API Key: \")\nIBM_CLOUD_URL= os.getenv(\"RUNTIME_ENV_APSX_URL\", \"https://us-south.ml.cloud.ibm.com\")\nPROJECT_ID = os.getenv(\"PROJECT_ID\")\n\nwml_creds = {\n    \"url\": IBM_CLOUD_URL,\n    \"apikey\": IBM_CLOUD_API_KEY\n}", "execution_count": 5, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Enter your IBM CLoud API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {}, "id": "fbe1c35a", "cell_type": "markdown", "source": "## 2. Index knowledge base"}, {"metadata": {}, "id": "dfc19dee", "cell_type": "markdown", "source": "### 2.1. Load data"}, {"metadata": {}, "id": "7317390d", "cell_type": "markdown", "source": "Select one of the two dataset available:\n1. *nq910* - an Information Retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\n2. *LongNQ* - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive question answering.\n\nThese datasets are provided under the /data directory."}, {"metadata": {}, "id": "4b0cf99c", "cell_type": "code", "source": "data_download_paths = {\n    'output.csv':'https://raw.githubusercontent.com/rich-nieto-ibm/techexchange_watsonx_workshop/main/output.csv',\n    'questions.csv':'https://raw.githubusercontent.com/rich-nieto-ibm/techexchange_watsonx_workshop/main/questions.csv'\n}\n\nfor file,url in data_download_paths.items():\n    if os.path.isfile(file) is False:\n        wget.download(url)\n    if os.path.isfile(file) is False:\n        raise IOError(f\"Failed to download {file}\")", "execution_count": 6, "outputs": []}, {"metadata": {}, "id": "afadc6c9", "cell_type": "code", "source": "questions = pd.read_csv(\"./questions.csv\").head(3000)\ndocuments = pd.read_csv(\"./output.csv\").head(3000)", "execution_count": 21, "outputs": []}, {"metadata": {}, "id": "6c5d904a", "cell_type": "code", "source": "dataset = 'LongNQ'", "execution_count": 22, "outputs": []}, {"metadata": {}, "id": "4d9fde6e", "cell_type": "code", "source": "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']", "execution_count": 23, "outputs": []}, {"metadata": {}, "id": "86442b2b", "cell_type": "markdown", "source": "#### 1.2. Create an embedding function\n\nNote that you can feed a custom embedding function to be used by chromadb. The performance of chromadb may differ depending on the embedding model used."}, {"metadata": {}, "id": "0072af6e", "cell_type": "code", "source": "from langchain.embeddings import HuggingFaceEmbeddings", "execution_count": 58, "outputs": []}, {"metadata": {}, "id": "cc9a25a2", "cell_type": "code", "source": "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")", "execution_count": 59, "outputs": []}, {"metadata": {}, "id": "5e5fe5fc", "cell_type": "raw", "source": "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n    def __call__(self, texts):\n        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\nemb_func = MiniLML6V2EmbeddingFunction()"}, {"metadata": {}, "id": "23e872fb", "cell_type": "markdown", "source": "#### 2.3. Set up Chroma upsert\nUpserting a document means update the document even if it exists in the database. Otherwise re-inserting a document throws an error. This is useful for experimentation purpose."}, {"metadata": {}, "id": "4b44fd95", "cell_type": "code", "source": "PERSIST_DIR = './storage'\nos.makedirs(PERSIST_DIR, exist_ok=True)\nchroma_client = chromadb.PersistentClient(PERSIST_DIR)", "execution_count": 60, "outputs": []}, {"metadata": {}, "id": "8c398b66", "cell_type": "code", "source": "documents_ = documents.indextext.tolist()\nids = [str(x) for x in documents.index.tolist()]", "execution_count": 26, "outputs": []}, {"metadata": {}, "id": "a8ba23d9", "cell_type": "code", "source": "from langchain.document_loaders.csv_loader import CSVLoader", "execution_count": 29, "outputs": []}, {"metadata": {}, "id": "69d93c5f", "cell_type": "code", "source": "documents", "execution_count": 39, "outputs": [{"output_type": "execute_result", "execution_count": 39, "data": {"text/plain": "        id                                               text  \\\n0        1  History of Idaho - wikipedia History of Idaho ...   \n1        2  1957 . Location Cataldo , Idaho Built 1848 Arc...   \n2        3  of the Columbia was created in June 1816 , and...   \n3        4  Canyon , he concluded that water transport was...   \n4        5  1842 , Father Pierre - Jean De Smet , with Fr....   \n...    ...                                                ...   \n2995  2996  by Punjabi University , ISBN 81 - 7380 - 778 -...   \n2996  2997  National Book Shop , 1994 . ISBN 81 - 7116 - 1...   \n2997  2998  e-Punjab Maharaja Ranjit Singh '' . External l...   \n2998  2999  Guru Angad Guru Amar Das Guru Ram Das Guru Arj...   \n2999  3000  History of Punjab Indian Sikhs 1780 births 183...   \n\n                 title                                          indextext  \n0     History of Idaho  History of Idaho\\nHistory of Idaho - wikipedia...  \n1     History of Idaho  History of Idaho\\n1957 . Location Cataldo , Id...  \n2     History of Idaho  History of Idaho\\nof the Columbia was created ...  \n3     History of Idaho  History of Idaho\\nCanyon , he concluded that w...  \n4     History of Idaho  History of Idaho\\n1842 , Father Pierre - Jean ...  \n...                ...                                                ...  \n2995      Ranjit Singh  Ranjit Singh\\nby Punjabi University , ISBN 81 ...  \n2996      Ranjit Singh  Ranjit Singh\\nNational Book Shop , 1994 . ISBN...  \n2997      Ranjit Singh  Ranjit Singh\\ne-Punjab Maharaja Ranjit Singh '...  \n2998      Ranjit Singh  Ranjit Singh\\nGuru Angad Guru Amar Das Guru Ra...  \n2999      Ranjit Singh  Ranjit Singh\\nHistory of Punjab Indian Sikhs 1...  \n\n[3000 rows x 4 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>title</th>\n      <th>indextext</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>History of Idaho - wikipedia History of Idaho ...</td>\n      <td>History of Idaho</td>\n      <td>History of Idaho\\nHistory of Idaho - wikipedia...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1957 . Location Cataldo , Idaho Built 1848 Arc...</td>\n      <td>History of Idaho</td>\n      <td>History of Idaho\\n1957 . Location Cataldo , Id...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>of the Columbia was created in June 1816 , and...</td>\n      <td>History of Idaho</td>\n      <td>History of Idaho\\nof the Columbia was created ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Canyon , he concluded that water transport was...</td>\n      <td>History of Idaho</td>\n      <td>History of Idaho\\nCanyon , he concluded that w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1842 , Father Pierre - Jean De Smet , with Fr....</td>\n      <td>History of Idaho</td>\n      <td>History of Idaho\\n1842 , Father Pierre - Jean ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>2996</td>\n      <td>by Punjabi University , ISBN 81 - 7380 - 778 -...</td>\n      <td>Ranjit Singh</td>\n      <td>Ranjit Singh\\nby Punjabi University , ISBN 81 ...</td>\n    </tr>\n    <tr>\n      <th>2996</th>\n      <td>2997</td>\n      <td>National Book Shop , 1994 . ISBN 81 - 7116 - 1...</td>\n      <td>Ranjit Singh</td>\n      <td>Ranjit Singh\\nNational Book Shop , 1994 . ISBN...</td>\n    </tr>\n    <tr>\n      <th>2997</th>\n      <td>2998</td>\n      <td>e-Punjab Maharaja Ranjit Singh '' . External l...</td>\n      <td>Ranjit Singh</td>\n      <td>Ranjit Singh\\ne-Punjab Maharaja Ranjit Singh '...</td>\n    </tr>\n    <tr>\n      <th>2998</th>\n      <td>2999</td>\n      <td>Guru Angad Guru Amar Das Guru Ram Das Guru Arj...</td>\n      <td>Ranjit Singh</td>\n      <td>Ranjit Singh\\nGuru Angad Guru Amar Das Guru Ra...</td>\n    </tr>\n    <tr>\n      <th>2999</th>\n      <td>3000</td>\n      <td>History of Punjab Indian Sikhs 1780 births 183...</td>\n      <td>Ranjit Singh</td>\n      <td>Ranjit Singh\\nHistory of Punjab Indian Sikhs 1...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000 rows \u00d7 4 columns</p>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "id": "2cf1dffd", "cell_type": "code", "source": "from langchain.document_loaders import DataFrameLoader\n\nloader = DataFrameLoader(documents, page_content_column=\"indextext\")\ndocuments = loader.load()", "execution_count": 55, "outputs": []}, {"metadata": {}, "id": "6f957a0b", "cell_type": "code", "source": "# Index the vector database by embedding then inserting document chunks\n# this is automatically tokenizing and embedding documents with given\n# embedding function. the documents are also stored.\nvectordb = Chroma.from_documents(documents=documents, \n                                 embedding=embeddings,\n                                 collection_name=dataset,\n                                 persist_directory=PERSIST_DIR,\n                                 collection_metadata=None,\n                                 ids=ids)\n# Save vector database as persistent files in the output folder\nvectordb.persist()", "execution_count": 62, "outputs": []}, {"metadata": {}, "id": "0f1789b5", "cell_type": "code", "source": "# maximal marginal relevance (MMR)\nretriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\ndoc_hits = retriever.get_relevant_documents(\"michael jackson\")\nfor doc in doc_hits:\n    print(doc.page_content)\n    #print(f'...from page {doc.metadata[\"page\"]}:\\n{doc.page_content}\\n')", "execution_count": 66, "outputs": [{"output_type": "stream", "text": "Shaquille O'Neal\nShaquille O'Neal - wikipedia Shaquille O'Neal Jump to : navigation , search `` Shaquille '' redirects here . For other people called Shaquille , see Shaquille ( disambiguation ) . Shaquille O'Neal O'Neal in 2011 ( 1972 - 03 - 06 ) March 6 , 1972 ( age 45 ) Newark , New Jersey Nationality American Listed height 7 ft 1 in ( 2.16 m ) Listed weight 325 lb ( 147 kg ) Career information High school Robert G. Cole ( San Antonio , Texas ) College LSU ( 1989 -- 1992 ) NBA draft 1992 / Round : 1 / Pick : 1st overall Selected by the Orlando Magic Playing career 1992 -- 2011 Position Center Number 32 , 34 , 33 , 36 Career history 1992 -- 1996 Orlando Magic 1996 -- 2004 Los Angeles Lakers 2004 -- 2008 Miami Heat 2008 -- 2009 Phoenix Suns 2009 -- 2010 Cleveland Cavaliers 2010 -- 2011 Boston Celtics Career highlights and awards 4 \u00d7 NBA champion ( 2000 -- 2002 , 2006 ) 3 \u00d7 NBA Finals MVP ( 2000 -- 2002 ) NBA Most Valuable Player ( 2000 ) 15 \u00d7 NBA All - Star ( 1993 -- 1998 , 2000 -- 2007 , 2009 ) 3 \u00d7 NBA All - Star Game MVP ( 2000 , 2004 , 2009 ) 8 \u00d7 All - NBA First Team ( 1998 , 2000 -- 2006 ) 2 \u00d7 All - NBA Second Team ( 1995 , 1999 ) 4\nBlack Velvet (song)\nchorus . Myles won the 1991 Grammy for Best Female Rock Vocal Performance for the song and the 1990 Juno Award for Single of the Year . Since its release , the power ballad has received substantial airplay , receiving a `` Millionaire Award '' from ASCAP in 2005 for more than four million radio plays . Contents ( hide ) 1 Background and writing 2 Music video 3 Release and reception 4 Track listings 5 Charts and sales 5.1 Weekly charts 5.2 Year - end charts 5.3 Certifications 6 Other versions 6.1 Chart performance / Robin Lee version 7 See also 8 References 9 External links Background and writing ( edit ) The song is a paean to Elvis Presley . Co-writer Christopher Ward , who was Myles ' then - boyfriend , was inspired on a bus full of Elvis fans riding to Memphis attending the 10th anniversary vigil at Graceland , in 1987 . Upon his return to Canada , he brought his idea to Alannah and producer David Tyson , who wrote the chords for the bridge . The song was one of three in a demo Myles presented to Atlantic Records which eventually got her signed to the label . Atlantic Records , much to the disappointment of Myles , for whom the song had been written , gave the song for country artist Robin Lee to record . In the USA , Myles ' version was released in December 1989 , while Lee 's\nShaquille O'Neal\n, to the Kazaam soundtrack . O'Neal was also featured in Aaron Carter 's 2001 hit single `` That 's How I Beat Shaq '' . Shaq also appears on the music video for the release . Shaquille O'Neal conducted the Boston Pops Orchestra at the Boston Symphony Hall on December 20 , 2010 . In July 2017 , O'Neal released a diss track aimed at LaVar Ball , the father of NBA point guard Lonzo Ball . The three minute song was released in response to Ball claiming him and his younger son LaMelo , would beat O'Neal and his son Shareef in a game of basketball . Acting Starting with Blue Chips and Kazaam , O'Neal appeared in movies that were panned by some critics . O'Neal is one of the first African Americans to portray a major comic book superhero in a motion picture , having starred as John Henry Irons , the protagonist in the 1997 film Steel . He is preceded only by Michael Jai White , whose film Spawn was released two weeks before Steel . O'Neal appeared as himself on an episode of Curb Your Enthusiasm , bedridden after Larry David 's character accidentally tripped him while stretching , and in two episodes each of My Wife and Kids and The Parkers . He appeared in cameo roles in the films , Freddy Got Fingered , Jack and Jill and Scary Movie 4 . O'Neal appeared in the 311 music video for the\n", "name": "stdout"}]}, {"metadata": {}, "id": "a6edb657", "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "1b176562", "cell_type": "markdown", "source": "#### 2.4 Embed and index documents with Chroma\nYou will now generate embeddings for the passages. This will take\n\nHowever if you want to full experience, then delete these files and rebuild them yourself. Note that creating the embeddings and indexes can take a long time. E.g. on a 2021 Macbook Pro, it took 45 mins to generate these files for the LongNQ dataset."}, {"metadata": {}, "id": "4ff51a88", "cell_type": "markdown", "source": "### 3. Generate a retrieval-augmented response to a question\n3.1. Instantiate watsonx model"}, {"metadata": {}, "id": "5dd7eaa2", "cell_type": "code", "source": "params = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MIN_NEW_TOKENS: 1,\n        GenParams.MAX_NEW_TOKENS: 100,\n        GenParams.TEMPERATURE: 0,\n    }\nmodel = Model(model_id='google/flan-ul2', params=params, credentials=creds, project_id=project_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "6535fadb", "cell_type": "markdown", "source": "#### 3.2. Select a question"}, {"metadata": {}, "id": "9d9c6323", "cell_type": "code", "source": "question_index = 65\nquestion_text = questions.text[question_index].strip(\"?\") + \"?\"\nprint(question_text)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "457ca61a", "cell_type": "markdown", "source": "#### 3.3. Retrieve relevant context\n"}, {"metadata": {}, "id": "040ac1bf", "cell_type": "code", "source": "relevant_chunks = chroma.query(\n    query_texts=[question_text],\n    n_results=5,\n)\nfor i, chunk in enumerate(relevant_chunks['documents'][0]):\n    print(\"=========\")\n    print(\"Paragraph index : \", relevant_chunks['ids'][0][i])\n    print(\"Paragraph : \", chunk)\n    print(\"Distance : \", relevant_chunks['distances'][0][i])", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "65d48afa", "cell_type": "markdown", "source": "#### 3.4. Feed the context and the question to `watsonx` model."}, {"metadata": {}, "id": "cf42f1a9", "cell_type": "code", "source": "def make_prompt(context, question_text):\n    return (f\"{context}\\n\\nPlease answer a question using this \"\n          + f\"text. \"\n          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n          + f\"Question: {question_text}\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "4d3207cd", "cell_type": "code", "source": "context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\nprompt = make_prompt(context, question_text)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "39cfea31", "cell_type": "code", "source": "response = model.generate_text(prompt)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "500b3614", "cell_type": "code", "source": "print(\"Question = \", question_text)\nprint(\"Answer = \", response)\nprint(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", questions.answers[question_index])", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "c45a302d", "cell_type": "markdown", "source": "### 4. Evaluate RAG performance on your data\nEvaluating the performance of your Generative AI system is critical to ensuring happy end users. However evaluation also requires having a test dataset. In this case, the top passages that shoudl be return for each question.\n\nNote that we want to evaluate the performance of both (1) the embedding function plus (2) how well the GenAI model summarizes the results.\n\nSo our test set must contain:\n\nThe indexes of the passage(s) that contain the answer - i.e. the goldstandard passages (if the question is answerable by the knowledge base)\nThe question's gold standard answer (this can be short or long-form)\n\n\n4.1. Evaluate the retrieval quality\nWere the correct passages returned via the similarity search functionality\n\nThere are many ways to compute retrieval quality, namely how the information contained in the documents that are relevant to the question being asked. We're focusing here on success at given number of returns (aka recall at given levels), which is to say, given a fixed number of documents returned (e.g., 1, 3, 5), is the question's answer contained in them. The scores increase with the recall level."}, {"metadata": {}, "id": "49901af8", "cell_type": "code", "source": "def compute_score(questions, answers, ranks=[1, 3, 5, 10], use_rouge=False, rouge_threshold=0.7):\n    \"\"\"\n    Computes the success at different levels of recall, given the goldstandard passage indexes per query.\n    It computes two scores:\n       * Success at rank_i, defined as sum_q 1_{top i answers for question q contains a goldstandard passage} / #questions\n       * Lenient success at rank i, defined as\n                sum_q 1_{ one in the documents in top i for question q contains a goldstandard answer) / #questions\n    Note that a document that contains the actual textual answer does not necesarily answer the question, hence it's a\n    more lenient evaluation. Any goldstandard passage will contain a goldstandard answer text, by definition.\n    Args:\n        :param questions: List[Dict['id': AnyStr, 'text': AnyStr, 'relevant': AnyStr, 'answers': AnyStr]]\n           - the input queries. Each query is a dictionary with the keys 'id','text', 'relevant', 'answers'.\n        :param input_passages: List[Dict['id': AnyStr, 'text': AnyStr', 'title': AnyStr]]\n           - the input passages. These are used to create a reverse-index list for the passages (so we can get the\n             text for a given passage ID)\n        :param answers: List[List[AnyStr]]\n           - the retrieved passages IDs for each query\n        :param ranks: List[int]\n           - the ranks at which to compute success\n        :param use_rouge: Boolean\n           - turns on the use of rouge as a scorer\n        :param rouge_threshold: float, default=0.7\n           - defines the minimum rouge-l/r score to accept the answer as a match,\n    Returns:\n\n\n    \"\"\"\n    # if \"relevant\" not in input_queries[0] or input_queries[0]['relevant'] is None:\n    #     print(\"The input question file does not contain answers. Please fix that and restart.\")\n    #     sys.exit(12)\n\n    scores = {r: 0 for r in ranks}\n    lscores = {r: 0 for r in ranks}\n\n    gt = {}\n    for q_relevant, q_qid in zip(questions.relevant, questions.qid):\n        if isinstance(q_relevant, str):\n            rel = [int(i) for i in q_relevant.split(\",\")]\n        else:\n            rel = [q_relevant]\n        gt[q_qid] = rel\n\n    def update_scores(ranks, rnk, scores):\n        j = 0\n        while j < len(ranks) and ranks[j] < rnk:\n            j += 1\n        for k in ranks[j:]:\n            scores[k] += 1\n\n    scorer = None\n    if use_rouge:\n        from rouge import Rouge\n        scorer = Rouge()\n\n    num_eval_questions = 0\n\n    for qi, (qid, q_answers) in enumerate(zip(questions.qid, questions.answers)):\n        tmp_scores = {r: 0 for r in ranks}\n\n        text_answers = str(q_answers).split(\"::\")\n        if \"-\" in text_answers:\n            # The question does not have answers, skip it for retrieval score purposes.\n            continue\n        num_eval_questions += 1\n        # Compute scores based on the goldstandard annotation\n        for ai, ans in enumerate(answers[qi]):\n            if int(ans['id']) in gt[qid]:  # Great, we found a match.\n                update_scores(ranks, ai + 1, tmp_scores)\n                break\n\n        # Compute score on approximate match - either answer inclusion in the text or\n        # minimum rouge score alignment.\n        tmp_lscores = tmp_scores.copy()  # making sure we're actually lenient\n        #inputq = questions[qi]\n        for ai, ans in enumerate(answers[qi]):\n            txt = ans['text'].lower()\n            found = False\n            for text_answer in text_answers:\n                if use_rouge:\n                    score = scorer.get_scores(text_answer.lower(), txt)\n                    if max(score[0]['rouge-l']['r'], score[0]['rouge-l']['p']) > rouge_threshold:\n                        update_scores(ranks, ai + 1, tmp_lscores)\n                        break\n                else:\n                    if not isinstance(text_answer, str):\n                        print(f\"Error on text_answer {text_answer}, question {qi}, answer {ai}-{ans}\")\n                    if txt.find(text_answer.lower()) >= 1:\n                        update_scores(ranks, ai + 1, tmp_lscores)\n                        break\n\n        for r in ranks:\n            scores[r] += int(tmp_scores[r] >= 1)\n            lscores[r] += int(tmp_lscores[r] >= 1)\n\n    res = {\"num_ranked_queries\": num_eval_questions,\n           \"num_judged_queries\": num_eval_questions,\n           \"success\":\n               {r: int(1000 * scores[r] / num_eval_questions) / 1000.0 for r in ranks},\n           \"lenient_success\":\n               {r: int(1000 * lscores[r] / num_eval_questions) / 1000.0 for r in ranks},\n           \"counts\": {r: scores[r] for r in ranks},\n           'lcounts': {r: lscores[r] for r in ranks}\n           }\n\n    return res", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "86269240", "cell_type": "markdown", "source": "#### Compute the retrieval score over all the documents\nCan take up to a minute"}, {"metadata": {}, "id": "12c5b9c9", "cell_type": "code", "source": "k = 5\nretrieved_docs = []\nfor q in questions.text:\n    answers = chroma.query(query_texts=q, n_results=k)\n\n    retrieved_docs.append([{'id': id, 'text': text}\n                           for (id, text) in zip(answers['ids'][0], answers['documents'][0])])\n\nres = compute_score(questions, retrieved_docs,\n                    ranks=[1, 3, 5], use_rouge=(data_dir == 'docs_and_qs'))\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0950159e", "cell_type": "code", "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef plot(res):\n    fig, ax = plt.subplots()\n    scores = res['success'].values()\n    keys = [f'R@{i}' for i in res['success'].keys()]\n    x_pos = np.arange(len(keys))\n    ax.bar(x_pos, scores, align='center', alpha=0.5)\n    ax.set_ylabel('Success Rate')\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(keys)\n    ax.set_title('Success rates at different recall rates.')\n    ax.yaxis.grid(True)\n\n    # Save the figure and show\n    plt.tight_layout()\n    plt.savefig('bar_plot.png')\n    plt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0d1f4e83", "cell_type": "code", "source": "plot(res)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "cbbf5079", "cell_type": "markdown", "source": "### 4.2. Evaluate quality of generated responses\nI.e. how well did the GenAI model summarize and extract the correct answer to the user's question from the passages returned by the similarity function.  Obviously if the returned passages were invalid, then performance at this phase would suffer too."}, {"metadata": {}, "id": "4c601a11", "cell_type": "markdown", "source": "##### Automatically evaluating the quality of answers is difficult, as many factors come into play, such as fluency, helpfulness, coverage, etc. One simplified way of computing this quality is using the ROUGE metric, in particular ROUGE-L. To compute this metric, for every answer returned for a question, we measure the maximum subsequence of words between the system answer and the gold-standard answer. Given this sequence, we can compute the precision of the given answer as the length (all lengths are in words) of this sequence divided by the length of the system answer and the recall as the length of the longest common subsequence divided by the length the gold-standard answer.\n$$ P_{ROUGE-L} = \\frac{|lcs(system,gold)|}{|system|} \\\\ R_{ROUGE_L} = \\frac{|lcs(system,gold|}{|gold|} $$\n\nwhere $lcs(system, gold)$ is the longest commong subsequence between $system$ and $gold$.\n\nROUGE was devised in the NLP community to evaluate summarization, and is commonly used to also evaluate abstractive question answering."}, {"metadata": {}, "id": "f23b22b6", "cell_type": "code", "source": "def score_answers(_answers, _reference, score_type=\"rouge-l\", val=\"r\", use_rouge=True):\n    \"\"\"\n    Compute the score of a set of answers, given a set of references, using Rouge score.\n    :param answers: Union[List[str], str]\n       - the returned answer/answers.\n    :param reference:\n        - the reference answers, in a string. Answers are separated by ':::'\n    :param use_rouge: Boolean\n        - if true, then use rouge for scoring, otherwise use substring.\n    :return:\n       - The maximum rouge-L score of the cartesian product of answers/references\n    \"\"\"\n    if isinstance(_answers, str):\n        _answers = [_answers]\n    _references = _reference.lower().split(\"::\")\n    max_score = -1\n    scorer = Rouge()\n    closest_ref = \"\"\n    for ref in _references:\n        for _answer in _answers:\n            if use_rouge:\n                scores = scorer.get_scores(_answer.lower(), ref)\n                score = scores[0][score_type][val]\n            else:\n                score = int(ref.find(_answer.lower()) >= 0)\n            if score > max_score:\n                max_score = score\n                closest_ref = ref\n\n    return max_score, closest_ref", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "8b48cb08", "cell_type": "code", "source": "print(\"Question = \", question_text)\nprint(\"Answer = \", response)\nscore, closest_ref = score_answers(response, questions.answers[question_index], val='r')\nprint(f\"Closest reference: \\\"{closest_ref}\\\"\")\nprint(f\"Recall:\\t\\t{100*score:5.2f}%\")\nscore, _ = score_answers(response, questions.answers[question_index], val='p')\nprint(f\"Precision:\\t{100*score:5.2f}%\")\n", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "5e5a4fda", "cell_type": "markdown", "source": "#### Compute (Rouge-based) precision and recall for the entire collection.\nIt takes about 1-2 seconds per question. For a corpus of ~1000 questions, this take can take up to 30min."}, {"metadata": {}, "id": "c7ca204a", "cell_type": "code", "source": "def is_answerable(relevant):\n    return \"-1\" in relevant", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "fa5db342", "cell_type": "code", "source": "rscore = 0\npscore = 0\nimport tqdm\nnum_eval_questions = 50\neval_questions = questions[:num_eval_questions]\ncount = {\"11\": 0, \"10\": 0, \"01\": 0, \"00\": 0}\nseq = []\nfor (question_text, answers, relevant) in tqdm.tqdm(zip(eval_questions.text, eval_questions.answers, eval_questions.relevant), total=len(eval_questions)):\n    # ans = qa(question.question)\n    relevant_chunks = chroma.query(\n        query_texts=[question_text],\n        n_results=5,\n    )\n    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n    prompt = make_prompt(context, question_text)\n    ans = model.generate_text(prompt)\n    q_answerable = is_answerable(relevant)\n    if ans == \"unanswerable\":\n        res = \"10\" if q_answerable else \"00\"\n        count [res] += 1\n        if not q_answerable:\n            rscore += 1\n            pscore += 1\n    else:\n        res = \"11\" if q_answerable else \"10\"\n        count[res] += 1\n        if q_answerable:\n            qrscore, _ = score_answers(ans, answers, val='r')\n            rscore += qrscore\n            qpscore, _ = score_answers(ans, answers, val='p')\n            pscore += qpscore\n    seq.append(res)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "6b12b258", "cell_type": "code", "source": "from IPython.display import HTML, display\ndef displayHTMLTables(*tables):\n    def htmlTable(table):\n        return '<table border=\"2\"><tr>{}</tr></table>'.format(\n                    '</tr><tr>'.join(\n                        '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in table)\n                )\n\n    display(HTML('<table><tr><td>{}</td></tr></table>'.format(\n                \"</td><td>\".join(htmlTable(table) for table in tables))\n))", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "c7f85bd6", "cell_type": "code", "source": "res = [['', 'Overall', 'Answerable questions'],\n       ['Precision', f\"{100*pscore/len(eval_questions):5.2f}\", f\"{100*(pscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n       ['Recall',    f'{100*pscore/len(eval_questions):5.2f}', f\"{188*(rscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n       ]\ncounts = [['Gold/System', 'No Answer', 'Answered'],\n        ['No Answer', count[\"00\"], count[\"01\"]],\n        ['With Answer', count[\"10\"], count[\"11\"]]]#%% md\n\ndisplayHTMLTables(res, [], [], counts)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0ac0348d", "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}